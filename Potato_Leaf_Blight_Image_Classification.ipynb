{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1ZKzPZXlkHs5"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXadn_h3yTG3"
      },
      "outputs": [],
      "source": [
        "# import needed libraries\n",
        "\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5VzlOCRy7I1"
      },
      "outputs": [],
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# define paths-\n",
        "#Done: update/add paths for your dataset\n",
        "data_path_train = '/content/drive/MyDrive/ACM_DAS/datasets/potato/train'\n",
        "data_path_test = '/content/drive/MyDrive/ACM_DAS/datasets/potato/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRX0IOmmyTei"
      },
      "outputs": [],
      "source": [
        "# Done: create train and test transforms https://docs.pytorch.org/vision/0.9/transforms.html\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEBo3pityY9W"
      },
      "outputs": [],
      "source": [
        "#Done: load the dataset\n",
        "train_dataset=datasets.ImageFolder(root=data_path_train, transform=train_transforms)\n",
        "test_dataset=datasets.ImageFolder(root=data_path_test, transform=test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Iu6JEuyPyY_I"
      },
      "outputs": [],
      "source": [
        "# Done: split into train/test datasets if needed\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# sample dummy image tensors\n",
        "image_data = torch.randn(1000, 3, 64, 64)\n",
        "labels = torch.randint(0, 10, (1000,))\n",
        "\n",
        "dataset = TensorDataset(image_data, labels)\n",
        "\n",
        "#Split into batches\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#to view every iterated batch\n",
        "for batch_images, batch_labels in dataloader:\n",
        "    print(f\"Batch shape: {batch_images.shape}, Labels: {batch_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLz0OeFNyZCA"
      },
      "outputs": [],
      "source": [
        "# update transforms if needed\n",
        "train_dataset.transform = train_transforms\n",
        "test_dataset.transform = test_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys7AWLpeyZEm"
      },
      "outputs": [],
      "source": [
        "# some info about the datasets\n",
        "print(\"Dataset classes: \", train_dataset.classes)\n",
        "print(\"Num train samples:\", len(train_dataset))\n",
        "print(\"Num test samples:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCTk5TfMyZGs"
      },
      "outputs": [],
      "source": [
        "\n",
        "BATCH_SIZE = 100;\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffle=True is crucial for training\n",
        "    num_workers=2\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False, # No need to shuffle test data\n",
        "    num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpeajO4B2po3"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "def denormalize(image_tensor):\n",
        "    img = image_tensor.clone()\n",
        "    img = img * std.view(-1, 1, 1) + mean.view(-1, 1, 1)\n",
        "    return img\n",
        "\n",
        "# Get one batch\n",
        "images, labels = next(iter(train_loader))\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "# Plot them\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(6):\n",
        "    ax = plt.subplot(2, 3, i + 1)\n",
        "    img = denormalize(images[i])\n",
        "    img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "    img_np = np.clip(img_np, 0, 1)\n",
        "    plt.imshow(img_np)\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Sample Train Images (Corrected)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wbzdmwRPyfpd"
      },
      "outputs": [],
      "source": [
        "# use block above to show images\n",
        "# show some images\n",
        "def show_sample_images(dataset, n):\n",
        "  fig, ax = plt.subplots(1, n, figsize=(12,3))\n",
        "  for i in range(n):\n",
        "    img, label = dataset[i]\n",
        "    img = (img/2) + 0.5\n",
        "    ax[i].imshow(img.permute(1,2,0))\n",
        "    ax[i].set_title(dataset.classes[label])\n",
        "    ax[i].axis('off')\n",
        "  plt.show()\n",
        "\n",
        "print(\"Sample train images: \")\n",
        "show_sample_images(train_dataset, 6)\n",
        "print(\"Sample test images: \")\n",
        "show_sample_images(test_dataset, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW-4J4KYyfrQ"
      },
      "outputs": [],
      "source": [
        "# show shape of dataloader\n",
        "for images, labels in train_loader:\n",
        "  print(\"Batch shape: \", images.shape)\n",
        "  print(\"Label shape: \", labels.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbM45QRd7psQ"
      },
      "outputs": [],
      "source": [
        "# define cnn model\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, 1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 3, 224, 224)\n",
        "            dummy_output = self.pool2(F.relu(self.conv2(self.pool(F.relu(self.conv1(dummy_input))))))\n",
        "            flattened_size = dummy_output.numel()\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbMRq_G_-bJS"
      },
      "outputs": [],
      "source": [
        "# define device, model, loss function, and optimizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN(num_classes=len(train_dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "devUCv99_ZBS"
      },
      "outputs": [],
      "source": [
        "# DONE: complete training loop\n",
        "\n",
        "for epoch in range(20):\n",
        "  model.train()\n",
        "  running_loss = 0.0;\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device) # Move data to the device\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs) # Use the initialized model instance\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 11 == 10:\n",
        "          print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "          running_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLlQLx-w_eNY"
      },
      "outputs": [],
      "source": [
        "# DONE: complete evaluation loop\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        images, labels = images.to(device), labels.to(device) # Move data to the device\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the {total} test images: {100 * correct / total} %')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBO-ZApnV-DO"
      },
      "outputs": [],
      "source": [
        "# to save model after training\n",
        "\n",
        "saved_model_path = '/content/drive/MyDrive/ACM_DAS/my_model.pt'\n",
        "torch.save(model.state_dict(), saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xXZrgtoV-JY"
      },
      "outputs": [],
      "source": [
        "# to load model again\n",
        "\n",
        "my_model = CNN(num_classes=len(train_dataset.classes))\n",
        "state_dict = torch.load(saved_model_path)\n",
        "my_model.load_state_dict(state_dict)\n",
        "my_model = my_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq6OzWRhXm69"
      },
      "outputs": [],
      "source": [
        "# gradio example\n",
        "\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import time\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVED_MODEL_PATH = '/content/drive/MyDrive/ACM_DAS/my_model.pt'\n",
        "\n",
        "CLASS_LABELS = [\n",
        "    \"Potato Early Blight\",\n",
        "    \"Potato Late Blight\",\n",
        "    \"Potato Healthy\"\n",
        "]\n",
        "\n",
        "IMG_SIZE = 224\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        dummy_input = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE)\n",
        "        dummy_output = self.pool2(F.relu(self.conv2(self.pool1(F.relu(self.conv1(dummy_input))))))\n",
        "        flattened_size = dummy_output.numel()\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "PREDICTION_TRANSFORMS = transforms.Compose([\n",
        "    transforms.Resize(256),       # Resize to 256\n",
        "    transforms.CenterCrop(IMG_SIZE), # Crop to 224x224\n",
        "    transforms.ToTensor(),        # Convert to tensor (0-1 float, C x H x W)\n",
        "    transforms.Normalize(         # Normalize with your specific mean/std\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "try:\n",
        "    num_classes = len(CLASS_LABELS)\n",
        "    # Initialize model on the device (CPU/GPU)\n",
        "    model = CNN(num_classes=num_classes).to(DEVICE)\n",
        "\n",
        "    # Load the state dictionary\n",
        "    state_dict = torch.load(SAVED_MODEL_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # Set model to evaluation mode (Crucial!)\n",
        "    model.eval()\n",
        "    print(f\"Successfully loaded model from {SAVED_MODEL_PATH} and set to {DEVICE}.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"--- ERROR: Model file not found! ---\")\n",
        "    print(f\"Please check the path: {SAVED_MODEL_PATH}\")\n",
        "    print(\"Using a dummy model for demonstration. Predictions will be random.\")\n",
        "    # Fallback to a dummy model if the file isn't found\n",
        "    class DummyModel(nn.Module):\n",
        "        def forward(self, x):\n",
        "            # Returns a random output tensor with the correct shape\n",
        "            return torch.rand(1, num_classes)\n",
        "    model = DummyModel().to(DEVICE)\n",
        "    model.eval()\n",
        "except Exception as e:\n",
        "    print(f\"--- ERROR loading model: {e} ---\")\n",
        "    # Fallback to a dummy model if loading fails\n",
        "    class DummyModel(nn.Module):\n",
        "        def forward(self, x):\n",
        "            # Returns a random output tensor with the correct shape\n",
        "            return torch.rand(1, num_classes)\n",
        "    model = DummyModel().to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def classify_potato_leaf(image: np.ndarray):\n",
        "    \"\"\"\n",
        "    The main prediction function called by Gradio.\n",
        "    It takes an image (as a NumPy array from Gradio) and returns a dictionary\n",
        "    of class probabilities for the gr.Label component.\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        return {\"Error, no image was uploaded\": 1.0}\n",
        "\n",
        "    # Simulate processing time to make it feel like a real model prediction\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    try:\n",
        "        # Convert NumPy array to a PIL Image,\n",
        "        if image.ndim == 2:\n",
        "            # Grayscale image (H x W) -> Convert to L (PIL Grayscale) -> Convert to RGB\n",
        "            pil_image = Image.fromarray(image.astype(np.uint8), 'L').convert('RGB')\n",
        "        elif image.ndim == 3 and image.shape[2] == 4:\n",
        "            # RGBA image (H x W x 4) -> Drop alpha channel\n",
        "            # Ensure proper scaling if input was float\n",
        "            if image.dtype in [np.float32, np.float64]:\n",
        "                 image = (image * 255).astype(np.uint8)\n",
        "            pil_image = Image.fromarray(image[:, :, :3], 'RGB')\n",
        "        else:\n",
        "            # Standard RGB (H x W x 3)\n",
        "            # Scale float arrays (0.0-1.0) to 0-255 before creating the PIL Image\n",
        "            if image.dtype in [np.float32, np.float64]:\n",
        "                 image = (image * 255).astype(np.uint8)\n",
        "            pil_image = Image.fromarray(image, 'RGB')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Image conversion error: {e}\")\n",
        "        return {\"Error: Could not convert image for model input\": 1.0}\n",
        "\n",
        "\n",
        "    # --- STEP B: PIL Image to PyTorch Tensor ---\n",
        "    # Apply your specific pre-processing steps defined in PREDICTION_TRANSFORMS\n",
        "    tensor_input = PREDICTION_TRANSFORMS(pil_image)\n",
        "\n",
        "    # Add a batch dimension (C x H x W) -> (1 x C x H x W)\n",
        "    tensor_input = tensor_input.unsqueeze(0)\n",
        "\n",
        "    # Move the tensor to the correct device (CPU or CUDA)\n",
        "    tensor_input = tensor_input.to(DEVICE)\n",
        "\n",
        "    # --- STEP C: GET PREDICTION ---\n",
        "    # Perform the forward pass without gradient calculations (evaluation mode)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tensor_input)\n",
        "\n",
        "    # 1. Convert logits to probabilities (Softmax)\n",
        "    probabilities = F.softmax(outputs, dim=1)[0]\n",
        "\n",
        "    # 2. Convert probabilities to a Python dictionary for Gradio's gr.Label\n",
        "    probabilities_dict = {\n",
        "        CLASS_LABELS[i]: probabilities[i].item() for i in range(len(CLASS_LABELS))\n",
        "    }\n",
        "\n",
        "    return probabilities_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "image_input = gr.Image(\n",
        "    type=\"numpy\",       # Gradio will convert the uploaded image to a NumPy array\n",
        "    label=\"Upload Potato Leaf Image\"\n",
        ")\n",
        "\n",
        "# Define the output component (Label for prediction results)\n",
        "output_label = gr.Label(\n",
        "    num_top_classes=3,  # Show all three class results\n",
        "    label=\"Model Prediction\"\n",
        ")\n",
        "\n",
        "# Create the Gradio Interface\n",
        "demo = gr.Interface(\n",
        "    fn=classify_potato_leaf,\n",
        "    inputs=image_input,\n",
        "    outputs=output_label,\n",
        "    title=\"Potato Leaf Disease Classifier (Gradio)\",\n",
        "    description=\"Upload an image of a potato leaf to get a prediction for Early Blight, Late Blight, or Healthy status.\"\n",
        ")\n",
        "\n",
        "# The following line starts the web app when the script is executed.\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}